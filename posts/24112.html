<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="IVAN论文笔记"><meta name="keywords" content=""><meta name="author" content="John Doe"><meta name="copyright" content="John Doe"><title>IVAN论文笔记 | Hexo</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.3.0'
} </script><meta name="generator" content="Hexo 6.3.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9"><span class="toc-number">2.</span> <span class="toc-text">创新点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C"><span class="toc-number">3.</span> <span class="toc-text">二</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89"><span class="toc-number">4.</span> <span class="toc-text">三</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#B-iVAN%E7%9A%84%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-number">4.1.</span> <span class="toc-text">B. iVAN的网络架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-iVAN%E7%9A%84%E8%AE%AD%E7%BB%83%E7%9B%AE%E6%A0%87%EF%BC%88%E5%AF%B9%E8%B1%A1%EF%BC%89"><span class="toc-number">4.2.</span> <span class="toc-text">C. iVAN的训练目标（对象）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-%E5%9B%BE%E5%83%8F%E5%90%88%E6%88%90%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">4.3.</span> <span class="toc-text">B. 图像合成的比较</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-number">5.</span> <span class="toc-text">测试</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1to1"><span class="toc-number">5.0.0.1.</span> <span class="toc-text">1to1</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#many-to-1"><span class="toc-number">5.0.0.1.1.</span> <span class="toc-text">many to 1</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://s2.loli.net/2023/03/31/Kl6pJgmODrqhzny.jpg"></div><div class="author-info__name text-center">John Doe</div><div class="author-info__description text-center"></div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/CasimiBreidin">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">9</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">3</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Hexo</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/archives">时间轴</a><a class="site-page" href="/categories">文章分类</a><a class="site-page" href="/about">关于我</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">IVAN论文笔记</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2022-10-19</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><div align='center'><font size='70'>Variable Augmented Network for Invertible
Modality Synthesis-Fusion</font></div>


<center><font>Yuhao Wang, Ruirui Liu, Zihao Li, Cailian Yang, Qiegen Liu</font></center>

<center><font>arXiv e-prints</font></center>

<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>提出的方法：可逆和可变的增广网络-IVAN（invertible and variable augmented network）</p>
<p>作用的领域：医学图像合成和融合（medical image synthesis and fusion）</p>
<p>作用：</p>
<p>可变增广技术（variable augmentation technology）→使网络输入和输出的通道数相同，增强了数据的相关性，利于表征信息的生成</p>
<p>可逆网络（invertible network）→实现双向推理过程</p>
<p>可逆和可变增广方案（invertible and variable augmentation schemes）→可用于多输入到单输出、多输入到多输出、单输入到多输出的映射</p>
<h2 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h2><p>1、可逆合成和融合</p>
<p>在统一框架中首次引入可逆网络，从而处理医学图像合成和融合这两个问题，进而形成一个可逆模态综合融合系统。</p>
<p>2、可变增广技术</p>
<p>使网络输入和输出的维度相同，从而降图像合成和图像融合作为单一的问题。</p>
<p>3、灵活性和多功能应用</p>
<p>不仅可用于多输入到单输出、多输入到多输出的映射，而且可用于单输入到多输出的映射。</p>
<h2 id="二"><a href="#二" class="headerlink" title="二"></a>二</h2><p>对于多模态合成和融合，都属于多输入-单输出的映射，即$\hat y &#x3D; f({x_1},{x_2},…,{x_n})$，其中$x$表示不同的模态图像，$\hat y$表示合成或融合图像。本文把每个输入模态视为一个通道，将它们堆叠起来作为IVAN的网络输入。</p>
<p>本文提出的医学合成和融合方法的基石：可逆神经网络INNs（Invertible neural networks）</p>
<p>公式：$\hat y &#x3D; f(x)$，对于一副图像$(x_i,y_i)$，$x$表示输入图像，$f$表示一个可逆网络，$y$表示输出图像，由于可逆性，对于${\rm{x}} &#x3D; {f^{ - 1}}(y)$同理。</p>
<p>要求：网络输入和输出的通道维度相同（可引入可变增广技术解决）</p>
<p>结构：</p>
<p><img src="C:\Users\Casimi_PC\AppData\Roaming\Typora\typora-user-images\image-20221009150735607.png" alt="image-20221009150735607"></p>
<h2 id="三"><a href="#三" class="headerlink" title="三"></a>三</h2><p>iVAN</p>
<p>维度相同的方法：添加虚拟变量（dummy variables）并将其复制到输出中</p>
<p>优点：</p>
<p>1、不会干扰模态独立特征结构，只学习模态之间的潜在相关性</p>
<p>2、提供了原始图像和目标图像之间的可逆连接</p>
<p>组成：模型设计（可逆块）是由一堆仿射耦合层（affine coupling layer）组成，在这些耦合层中，利用可逆的1X1卷积作为可学习置换函数</p>
<p>训练过程：</p>
<p><img src="C:\Users\Casimi_PC\AppData\Roaming\Typora\typora-user-images\image-20221010104605385.png" alt="image-20221010104605385"></p>
<p>利用切片后的二维图像进行训练，对图像进行切片，并对其进行归一化处理，以获得更多的数据信息，提高数据的匹配度。</p>
<p>然后对输入数据进行数据增强处理（使其更加完整），</p>
<p>之后进入到可逆网络模型中训练。</p>
<p>&#x3D;&#x3D;在训练过程中，对于图像合成：原始模态图像作为网络输入，目标模态图像为作为标签；对于图像融合，不同的模态图像作为网络输入，目标融合图像作为标签。&#x3D;&#x3D;</p>
<h3 id="B-iVAN的网络架构"><a href="#B-iVAN的网络架构" class="headerlink" title="B. iVAN的网络架构"></a>B. iVAN的网络架构</h3><p>目标：找到一个双射函数，使得数据点从输入数据空间X映射到输出数据空间Y</p>
<p>用一堆可逆和可处理的双射函数$\left{ {{f_i}} \right}_{i &#x3D; 0}^k$来设计iVAN，即$y &#x3D; {f_0} \cdot {f_1} \cdot {f_2} \cdots {f_k}(x)$，其中$x$表示给定的观测数据样本，$y$表示目标数据样本。$f_i$表示双射模型（即下面的可逆块）。</p>
<p>详细结构：</p>
<p><img src="C:\Users\Casimi_PC\AppData\Roaming\Typora\typora-user-images\image-20221010193047561.png" alt="image-20221010193047561"></p>
<p>iVAN由一些可逆块（invertible block，图中橙色的方块）组成，</p>
<p>可逆块由可逆的1X1卷积核和仿射耦合层组成，</p>
<p>组成：模型设计是由一堆仿射耦合层（affine coupling layer）组成，在这些耦合层中，利用可逆的1X1卷积作为可学习置换函数</p>
<h3 id="C-iVAN的训练目标（对象）"><a href="#C-iVAN的训练目标（对象）" class="headerlink" title="C. iVAN的训练目标（对象）"></a>C. iVAN的训练目标（对象）</h3><p>公式：</p>
<p>${L_{total}} &#x3D; \lambda {L_1} + {L_2} &#x3D; \lambda {\left| {f(X) - \left. Y \right|} \right._2} + \left| {{f^{ - 1}}(Y) - \left. X \right|} \right.$</p>
<p>$Y$：带标签的目标图像</p>
<p>$f(X)$：原始图像X经过iVAN网络后输出的图像</p>
<p>${\left| {\left. {} \right|} \right._2}$$：$${L_2}$的范数</p>
<p>${{\cal L}_1}$：合成图像和带标签的目标图像的损失函数（代码中名为rgb_loss）

${{\cal L}_2}$：输入图像和参考图像的损失函数（raw_loss）

$\lambda $：超参数，用于平衡两个损失(weight)

使用欧几里得损失生成代表性特征，损失函数使输入的像素值与合成和融合图像之间的均方误差最小化。

### D. iVAN的“对抗性学习”的解释

![image-20221012194422367](C:\Users\Casimi_PC\AppData\Roaming\Typora\typora-user-images\image-20221012194422367.png)

$x$：输入图像（T1或者PD）

$G$：operator（算子）

$\tilde y$：transformed synthesis image（转换后的合成图像）

$Loss 1$：${\left\| {G(x) - \left. y \right\|} \right._1}$

$y$：reference synthesis image（参考的合成图像）

$D$：discriminative network（判别网络）

$Loss 2$：${\left\| {D(\left. {G(x)) - D(y)} \right\|} \right._1}$

首先通过算子G获得$\tilde y$，然后在判别网络D下$\tilde y$（即为$G(x)$）和$y$进行比较，即为${\left\| {D(\left. {G(x)) - D(y)} \right\|} \right._1}$。

![image-20221012202339497](C:\Users\Casimi_PC\AppData\Roaming\Typora\typora-user-images\image-20221012202339497.png)

## 四：实验结果

iVAN架构：使用二维轴平面切片作为网络输入

训练次数为300次，其中前50次的学习率为0.0001，之后每进行50次，学习率都减半（halved），权衡参数$\lambda $（trade-off parameter）为1。

质量指标：

在合成实验中，采用

峰值信噪比$PSNR(y,\tilde y) = 200{\log _{10}}Max(y)&#x2F;{\left| {\left. {\tilde y - y} \right|} \right._2}$</p>
<p>结构相似性指标$SSIM(y,\tilde y) &#x3D; {{(2{\mu _y}{\mu _{\tilde y}} + {c_1})(2{\sigma _{y\tilde y}} + {c_2})} \over {(\mu _y^2 + \mu _{\tilde y}^2 + c1)(\sigma _y^2 + \sigma _{\tilde y}^2 + c2)}}$</p>
<p>归一化均方误差$NMSE(y,\tilde y) &#x3D; \left| {\left. {y - \tilde y} \right|_2^2} \right.&#x2F;\left| {\left. y \right|} \right._2^2$</p>
<p>其中$y$为已合成的图像，$\tilde y$为带标签的目标图像</p>
<p>在融合实验中，采用</p>
<p>平均梯度$AG &#x3D; {1 \over {H \times W}}\sum\limits_{h &#x3D; 1}^H {\sum\limits_{w &#x3D; 1}^W {\sqrt {{X_h}{{(h,w)}^2} + {X_w}{{(h,w)}^2}} } } $，用于评估图像的局部对比度</p>
<p>空间频率$SF &#x3D; \sqrt {{{(RF)}^2} + {{(CF)}^2}} $，表示融合医学图像所拥有的信息水平</p>
<p>（EN）熵$H(A) &#x3D;  - \sum\limits_{l &#x3D; 0}^{L - 1} {{P_x}(l){{\log }_2}{P_x}(l)} $，用于衡量图像包含多少信息

${Q_{MI}} &#x3D; 2\left[ {{{MI({x_1},\tilde y)} \over {H({x_1}) + H(\tilde y)}} + {{MI({x_2},\tilde y)} \over {H({x_2}) + H(\tilde y)}}} \right]$，反映输入原始图像和融合图像之间的相关性</p>
<p>${Q_{ab&#x2F;f}} &#x3D; {1 \over {|W|}}\sum\limits_{\omega  \in W}^{} {\left[ {\lambda (\omega ){Q_0}({x_1},\tilde y|\omega ) + (1 - \lambda (\omega )){Q_0}({x_2},\tilde y|\omega )} \right]} $</p>
<h3 id="B-图像合成的比较"><a href="#B-图像合成的比较" class="headerlink" title="B. 图像合成的比较"></a>B. 图像合成的比较</h3><p>T1&#x3D;&gt;T2：图6，尽管四种方法产生了高质量的合成T2图像，但iVAN展示出更加锋利的边缘。</p>
<p>T1+PD&#x3D;&gt;T2：图7，iVAN在多模态&#x3D;&gt;单模态仍比Hi-Net好，更锋利的边缘和更低的模糊伪影。</p>
<p>T1+T2&#x3D;&gt;PET：又拿个多模态输入实验表明，多模态输入比单模态输入好</p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>python ..&#x2F;input&#x2F;ivaninclude-data&#x2F;VAN-(T1+PD)2T2-l2&#x2F;train.py –task&#x3D;2to1 –out_path&#x3D;”.&#x2F;kaggle&#x2F;working&#x2F;exps&#x2F;“</p>
<p>python ..&#x2F;input&#x2F;ivaninclude-data&#x2F;VAN-(T1+PD)2T2-l2&#x2F;train.py –task&#x3D;1to1 –out_path&#x3D;”.&#x2F;exps&#x2F;“</p>
<p>..&#x2F;input&#x2F;ivancode&#x2F;VAN-(T1+PD)2T2-l2</p>
<p>python train.py –task&#x3D;1to1 –out_path&#x3D;”&#x2F;kaggle&#x2F;working&#x2F;exps”</p>
<p>—————–杂项，可以在谷歌云跑代码了，但没必要———————–</p>
<p>pip install scikit-image&#x3D;&#x3D;0.16.2 -i <a target="_blank" rel="noopener" href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></p>
<p>import skimage<br>print(skimage.<strong>version</strong>)</p>
<p>opencv-python&#x3D;&#x3D;4.5.3.56</p>
<p>pip install tensorboardX&#x3D;&#x3D;1.7.0</p>
<p>在虚拟环境的文件夹中打开终端，进入该环境，直接执行1to1，2to1训练代码</p>
<p>目前：xxx和NCSN++和TV</p>
<p>1、先连接到云盘（因为文件夹上传到谷歌云盘上）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from google.colab import drive</span><br><span class="line">drive.mount(&#x27;/content/drive&#x27;)</span><br></pre></td></tr></table></figure>

<p>弹窗选择“允许”</p>
<p>2、进入要训练的文件夹内</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /content/drive/MyDrive/iVAN_Google/</span><br></pre></td></tr></table></figure>

<p>验证是否进入到文件夹内</p>
<p><code>ls</code></p>
<p>3、直接跳到4运行下，看提示缺少什么包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># 降级scikit-image包</span><br><span class="line">pip uninstall scikit-image</span><br><span class="line"></span><br><span class="line">pip install scikit-image==0.16.2 -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">os.kill(os.getpid(), 9)</span><br><span class="line"></span><br><span class="line">import skimage</span><br><span class="line">print(skimage.__version__)</span><br><span class="line"></span><br><span class="line"># 安装tensorboardX包</span><br><span class="line"></span><br><span class="line">pip install tensorboardX==1.7.0</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">os.kill(os.getpid(), 9)</span><br><span class="line"></span><br><span class="line"># 安装cv2需要的版本，之前自带的4.6，所以训练不了</span><br><span class="line">pip uninstall opencv-python</span><br><span class="line"></span><br><span class="line">pip install opencv-python==4.5.3.56</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">os.kill(os.getpid(), 9)</span><br><span class="line"></span><br><span class="line">import cv2</span><br><span class="line">print(cv2.__version__)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>训练和测试的代码，要加!</p>
<p>训练</p>
<h5 id="1to1"><a href="#1to1" class="headerlink" title="1to1"></a>1to1</h5><p>!python train.py –task&#x3D;1to1 –out_path&#x3D;”.&#x2F;exps&#x2F;“</p>
<h6 id="many-to-1"><a href="#many-to-1" class="headerlink" title="many to 1"></a>many to 1</h6><p>!python train.py –task&#x3D;2to1 –out_path&#x3D;”.&#x2F;exps&#x2F;“</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">John Doe</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/posts/24112.html">http://example.com/posts/24112.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com">Hexo</a>！</span></div></div><div class="post-meta__tag-list"></div><nav id="pagination"><div class="prev-post pull-left"><a href="/posts/57164.html"><i class="fa fa-chevron-left">  </i><span>ResNet论文笔记</span></a></div><div class="next-post pull-right"><a href="/posts/41853.html"><span>《PyTorch深度学习实践》- 刘二大人p7 - 处理多维特征的输入</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2022 - 2023 By John Doe</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/lib/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>